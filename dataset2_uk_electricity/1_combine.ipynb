{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4fee5d5",
   "metadata": {},
   "source": [
    "# 1_combine.ipynb  \n",
    "### Team: Team Yunus\n",
    "### Made by: Yunus Eren Ertas\n",
    "\n",
    "This notebook loads all the raw NESO electricity demand CSV files, detects the \n",
    "correct timestamp and demand columns for each year, and combines everything \n",
    "into one unified dataset.\n",
    "\n",
    "Because NESO changes file formats frequently (column names, datetime formats, \n",
    "etc.), a robust loader is needed. The output of this notebook is a single \n",
    "`uk_electricity_combined.csv` file that will be used in the cleaning stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44528d6f",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "\n",
    "- Imports the required Python libraries.\n",
    "- Sets the path to the folder where all raw NESO CSV files are stored.\n",
    "- Lists all files that will be processed.\n",
    "- This helps us confirm that all expected yearly files (2001–2025) are present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "062b686d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 25\n",
      "- demanddata_2001.csv\n",
      "- demanddata_2002.csv\n",
      "- demanddata_2003.csv\n",
      "- demanddata_2004.csv\n",
      "- demanddata_2005.csv\n",
      "- demanddata_2006.csv\n",
      "- demanddata_2007.csv\n",
      "- demanddata_2008.csv\n",
      "- demanddata_2009.csv\n",
      "- demanddata_2010.csv\n",
      "- demanddata_2011.csv\n",
      "- demanddata_2012.csv\n",
      "- demanddata_2013.csv\n",
      "- demanddata_2014.csv\n",
      "- demanddata_2015.csv\n",
      "- demanddata_2016.csv\n",
      "- demanddata_2017.csv\n",
      "- demanddata_2018.csv\n",
      "- demanddata_2019.csv\n",
      "- demanddata_2020.csv\n",
      "- demanddata_2021.csv\n",
      "- demanddata_2022.csv\n",
      "- demanddata_2023.csv\n",
      "- demanddata_2024.csv\n",
      "- demanddata_2025.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path(\"../data/raw/electricity\")\n",
    "files = sorted(root.glob(\"*.csv\"))\n",
    "\n",
    "print(\"Number of files:\", len(files))\n",
    "for f in files:\n",
    "    print(\"-\", f.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9e2b26",
   "metadata": {},
   "source": [
    "### Define a robust CSV loader for NESO files\n",
    "\n",
    "NESO changes column names and timestamp formats across years.  \n",
    "This function:\n",
    "\n",
    "- Normalizes column names  \n",
    "- Searches multiple possible timestamp columns  \n",
    "- Searches multiple possible demand columns (priority order)  \n",
    "- Converts both fields to correct types  \n",
    "- Returns only `timestamp` and `demand_mw`  \n",
    "\n",
    "If a file lacks valid timestamp/demand columns, it is skipped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3d6a228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_neso_csv(path):\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Normalize column names\n",
    "    df.columns = [c.lower().strip().replace(\" \", \"_\") for c in df.columns]\n",
    "\n",
    "    # --------------------\n",
    "    # CASE 1: Settlement period data (old NESO format)\n",
    "    # --------------------\n",
    "    if \"settlement_date\" in df.columns and \"settlement_period\" in df.columns:\n",
    "        \n",
    "        df[\"settlement_date\"] = pd.to_datetime(df[\"settlement_date\"], errors=\"coerce\")\n",
    "\n",
    "        # Construct timestamp:\n",
    "        # period 1 = 00:00\n",
    "        # period 2 = 00:30\n",
    "        # ...\n",
    "        df[\"timestamp\"] = df[\"settlement_date\"] + pd.to_timedelta(\n",
    "            (df[\"settlement_period\"] - 1) * 30, unit=\"m\"\n",
    "        )\n",
    "\n",
    "    # --------------------\n",
    "    # CASE 2: Already has a timestamp column (newer NESO format)\n",
    "    # --------------------\n",
    "    else:\n",
    "        time_candidates = [\"datetime\", \"local_time\", \"time\", \"timestamp\"]\n",
    "        tcol = None\n",
    "        for c in time_candidates:\n",
    "            match = [x for x in df.columns if c in x]\n",
    "            if match:\n",
    "                tcol = match[0]\n",
    "                break\n",
    "\n",
    "        if tcol is None:\n",
    "            print(f\"⚠ No usable timestamp in {path.name}, skipping.\")\n",
    "            return None\n",
    "\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[tcol], errors=\"coerce\")\n",
    "\n",
    "    # --------------------\n",
    "    # Demand column detection\n",
    "    # --------------------\n",
    "    demand_priority = [\n",
    "        \"england_wales_demand\",  # best\n",
    "        \"actual_demand\",\n",
    "        \"nd\",\n",
    "        \"tsd\",\n",
    "        \"demand\"\n",
    "    ]\n",
    "\n",
    "    dcol = None\n",
    "    for k in demand_priority:\n",
    "        match = [c for c in df.columns if k in c]\n",
    "        if match:\n",
    "            dcol = match[0]\n",
    "            break\n",
    "\n",
    "    if dcol is None:\n",
    "        print(f\"⚠ No demand in {path.name}, skipping.\")\n",
    "        return None\n",
    "\n",
    "    df[dcol] = pd.to_numeric(df[dcol], errors=\"coerce\")\n",
    "\n",
    "    out = df[[\"timestamp\", dcol]].dropna()\n",
    "    out.columns = [\"timestamp\", \"demand_mw\"]\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec079b31",
   "metadata": {},
   "source": [
    "### Load all files using the robust loader\n",
    "\n",
    "Each file is processed individually.  \n",
    "If valid timestamp and demand columns are found, the file is added to `frames`.  \n",
    "Otherwise, the file is skipped safely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e6c9c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: demanddata_2001.csv\n",
      "Reading: demanddata_2002.csv\n",
      "Reading: demanddata_2003.csv\n",
      "Reading: demanddata_2004.csv\n",
      "Reading: demanddata_2005.csv\n",
      "Reading: demanddata_2006.csv\n",
      "Reading: demanddata_2007.csv\n",
      "Reading: demanddata_2008.csv\n",
      "Reading: demanddata_2009.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erene\\AppData\\Local\\Temp\\ipykernel_4840\\1919508322.py:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"settlement_date\"] = pd.to_datetime(df[\"settlement_date\"], errors=\"coerce\")\n",
      "C:\\Users\\erene\\AppData\\Local\\Temp\\ipykernel_4840\\1919508322.py:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"settlement_date\"] = pd.to_datetime(df[\"settlement_date\"], errors=\"coerce\")\n",
      "C:\\Users\\erene\\AppData\\Local\\Temp\\ipykernel_4840\\1919508322.py:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"settlement_date\"] = pd.to_datetime(df[\"settlement_date\"], errors=\"coerce\")\n",
      "C:\\Users\\erene\\AppData\\Local\\Temp\\ipykernel_4840\\1919508322.py:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"settlement_date\"] = pd.to_datetime(df[\"settlement_date\"], errors=\"coerce\")\n",
      "C:\\Users\\erene\\AppData\\Local\\Temp\\ipykernel_4840\\1919508322.py:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"settlement_date\"] = pd.to_datetime(df[\"settlement_date\"], errors=\"coerce\")\n",
      "C:\\Users\\erene\\AppData\\Local\\Temp\\ipykernel_4840\\1919508322.py:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"settlement_date\"] = pd.to_datetime(df[\"settlement_date\"], errors=\"coerce\")\n",
      "C:\\Users\\erene\\AppData\\Local\\Temp\\ipykernel_4840\\1919508322.py:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"settlement_date\"] = pd.to_datetime(df[\"settlement_date\"], errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: demanddata_2010.csv\n",
      "Reading: demanddata_2011.csv\n",
      "Reading: demanddata_2012.csv\n",
      "Reading: demanddata_2013.csv\n",
      "Reading: demanddata_2014.csv\n",
      "Reading: demanddata_2015.csv\n",
      "Reading: demanddata_2016.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erene\\AppData\\Local\\Temp\\ipykernel_4840\\1919508322.py:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"settlement_date\"] = pd.to_datetime(df[\"settlement_date\"], errors=\"coerce\")\n",
      "C:\\Users\\erene\\AppData\\Local\\Temp\\ipykernel_4840\\1919508322.py:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"settlement_date\"] = pd.to_datetime(df[\"settlement_date\"], errors=\"coerce\")\n",
      "C:\\Users\\erene\\AppData\\Local\\Temp\\ipykernel_4840\\1919508322.py:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"settlement_date\"] = pd.to_datetime(df[\"settlement_date\"], errors=\"coerce\")\n",
      "C:\\Users\\erene\\AppData\\Local\\Temp\\ipykernel_4840\\1919508322.py:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"settlement_date\"] = pd.to_datetime(df[\"settlement_date\"], errors=\"coerce\")\n",
      "C:\\Users\\erene\\AppData\\Local\\Temp\\ipykernel_4840\\1919508322.py:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"settlement_date\"] = pd.to_datetime(df[\"settlement_date\"], errors=\"coerce\")\n",
      "C:\\Users\\erene\\AppData\\Local\\Temp\\ipykernel_4840\\1919508322.py:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"settlement_date\"] = pd.to_datetime(df[\"settlement_date\"], errors=\"coerce\")\n",
      "C:\\Users\\erene\\AppData\\Local\\Temp\\ipykernel_4840\\1919508322.py:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"settlement_date\"] = pd.to_datetime(df[\"settlement_date\"], errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: demanddata_2017.csv\n",
      "Reading: demanddata_2018.csv\n",
      "Reading: demanddata_2019.csv\n",
      "Reading: demanddata_2020.csv\n",
      "Reading: demanddata_2021.csv\n",
      "Reading: demanddata_2022.csv\n",
      "Reading: demanddata_2023.csv\n",
      "Reading: demanddata_2024.csv\n",
      "Reading: demanddata_2025.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erene\\AppData\\Local\\Temp\\ipykernel_4840\\1919508322.py:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"settlement_date\"] = pd.to_datetime(df[\"settlement_date\"], errors=\"coerce\")\n",
      "C:\\Users\\erene\\AppData\\Local\\Temp\\ipykernel_4840\\1919508322.py:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"settlement_date\"] = pd.to_datetime(df[\"settlement_date\"], errors=\"coerce\")\n"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "\n",
    "for f in files:\n",
    "    print(\"Reading:\", f.name)\n",
    "    df_part = load_neso_csv(f)\n",
    "    if df_part is not None:\n",
    "        frames.append(df_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50de4bd",
   "metadata": {},
   "source": [
    "### Combine all valid partial DataFrames into one dataset\n",
    "\n",
    "- Concatenates all data into a single DataFrame  \n",
    "- Removes remaining invalid rows  \n",
    "- Sorts chronologically  \n",
    "- Displays the first rows for verification  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8759e6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>demand_mw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001-01-01 00:00:00</td>\n",
       "      <td>34060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001-01-01 00:30:00</td>\n",
       "      <td>35370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-01-01 01:00:00</td>\n",
       "      <td>35680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001-01-01 01:30:00</td>\n",
       "      <td>35029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001-01-01 02:00:00</td>\n",
       "      <td>34047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp  demand_mw\n",
       "0 2001-01-01 00:00:00      34060\n",
       "1 2001-01-01 00:30:00      35370\n",
       "2 2001-01-01 01:00:00      35680\n",
       "3 2001-01-01 01:30:00      35029\n",
       "4 2001-01-01 02:00:00      34047"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.concat(frames, ignore_index=True)\n",
    "df = df.dropna().sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a59ff2",
   "metadata": {},
   "source": [
    "### Why this check is important\n",
    "\n",
    "Each year should have either:\n",
    "- **17,520 rows** (365 days × 48 half-hours)\n",
    "- **17,568 rows** (leap year = 366 days × 48 half-hours)\n",
    "- **Partial number** only if the year is not complete (e.g., 2025)\n",
    "\n",
    "This confirms:\n",
    "- All years loaded correctly.\n",
    "- No missing years.\n",
    "- No corrupted timestamp parsing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "655f7d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year\n",
       "2001    17520\n",
       "2002    17520\n",
       "2003    17520\n",
       "2004    17568\n",
       "2005    17520\n",
       "2006    17520\n",
       "2007    17520\n",
       "2008    17568\n",
       "2009    17520\n",
       "2010    17520\n",
       "2011    17520\n",
       "2012    17568\n",
       "2013    17520\n",
       "2014    17520\n",
       "2015    17520\n",
       "2016    17568\n",
       "2017    17520\n",
       "2018    17520\n",
       "2019    17520\n",
       "2020    17568\n",
       "2021    17520\n",
       "2022    17520\n",
       "2023    17520\n",
       "2024    17568\n",
       "2025    14640\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"year\"] = df[\"timestamp\"].dt.year\n",
    "df[\"year\"].value_counts().sort_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8996e841",
   "metadata": {},
   "source": [
    "### Final step\n",
    "\n",
    "We save the combined dataset in a clean 2-column CSV:\n",
    "- `timestamp`\n",
    "- `demand_mw`\n",
    "\n",
    "This file will be used in the next notebook (`02-clean.ipynb`) where we will:\n",
    "- Resample to hourly\n",
    "- Handle missing values\n",
    "- Remove invalid spikes\n",
    "- Export the final cleaned dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d91e4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined file to: ..\\data\\raw\\uk_electricity_combined.csv\n"
     ]
    }
   ],
   "source": [
    "out_path = Path(\"../data/raw/uk_electricity_combined.csv\")\n",
    "df[[\"timestamp\", \"demand_mw\"]].to_csv(out_path, index=False)\n",
    "\n",
    "print(\"Saved combined file to:\", out_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
