{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34870975",
   "metadata": {},
   "source": [
    "# 6_aws.ipynb\n",
    "### Team: *Team Yunus*\n",
    "### Made By: *Yunus Eren Ertas*\n",
    "## 1️⃣ Setup SageMaker Session & Imports — Technical Explanation\n",
    "\n",
    "This block imports all necessary AWS and data-science libraries used for training an XGBoost model on SageMaker.\n",
    "\n",
    "Modules included:\n",
    "\n",
    "- sagemaker – top-level SDK for launching training jobs, creating models, and running batch transform.\n",
    "\n",
    "- get_execution_role() – retrieves the IAM role assigned to the notebook/instance so SageMaker can access S3.\n",
    "\n",
    "- TrainingInput – wrapper specifying how SageMaker should read training data (format, S3 path).\n",
    "\n",
    "- get_image_uri() – retrieves the correct XGBoost training container image for the selected AWS region.\n",
    "\n",
    "- boto3 – direct AWS SDK for interacting with S3 buckets.\n",
    "\n",
    "- pandas / numpy – local processing utilities.\n",
    "\n",
    "- json – useful for parameter serialization.\n",
    "\n",
    "The final lines initialize:\n",
    "\n",
    "- session → primary SageMaker session object used for S3 uploads and job orchestration.\n",
    "\n",
    "- bucket → your S3 bucket where data + model outputs will be stored.\n",
    "\n",
    "- prefix → sub-folder for organizing project files.\n",
    "\n",
    "- region → AWS region where SageMaker runs.\n",
    "\n",
    "- role → IAM role assigned to SageMaker for permissions.\n",
    "\n",
    "Printing the region and role confirms correct AWS configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6870bc0-9129-4e97-8477-01296a43cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c7d1bd-24de-44f8-bf40-7d8474474ea2",
   "metadata": {},
   "source": [
    "## AWS Session & Environment Setup — Technical Explanation\n",
    "\n",
    "This block initializes the SageMaker environment and sets up all required AWS configuration values.\n",
    "\n",
    "- session = sagemaker.Session()\n",
    "- Creates a SageMaker session which manages S3 uploads, training jobs, and model storage.\n",
    "\n",
    "- bucket = \"my-cloud-ai-bucket\"\n",
    "- Defines the S3 bucket where training data, test data, and output artifacts will be stored.\n",
    "\n",
    "- prefix = \"electricity\"\n",
    "- Sets a subfolder inside the S3 bucket to keep all project-related files organized.\n",
    "\n",
    "- region = session.boto_region_name\n",
    "- Fetches the AWS region in which the notebook and SageMaker services are running.\n",
    "\n",
    "- role = get_execution_role()\n",
    "- Retrieves the IAM role that grants SageMaker permission to access S3 and run training jobs.\n",
    "\n",
    "- Print statements\n",
    "- These display the detected region and IAM role, helping verify that AWS is configured correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "128084e7-9657-4375-97ab-f4384dbbfd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-east-1\n",
      "Role: arn:aws:iam::711696934160:role/LabRole\n"
     ]
    }
   ],
   "source": [
    "session = sagemaker.Session()\n",
    "\n",
    "bucket = \"my-cloud-ai-bucket\"       # <-- your bucket\n",
    "prefix = \"electricity\"              # folder\n",
    "region = session.boto_region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "print(\"Region:\", region)\n",
    "print(\"Role:\", role)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363efd35-9a19-4c6e-96e2-15eb461ddd9a",
   "metadata": {},
   "source": [
    "## Upload Training & Test Data to S3 — Technical Explanation\n",
    "\n",
    "This block uploads the local training and test CSV files into your S3 bucket so SageMaker can access them during training and batch inference. SageMaker cannot read files from your local machine — all data must be stored in S3 first.\n",
    "\n",
    "- session.upload_data()\n",
    "Uploads a local file to an S3 bucket under the specified key prefix (folder).\n",
    "\n",
    "- Training dataset upload\n",
    "The file electricity_train.csv is uploaded to:\n",
    "s3://<bucket>/<prefix>/electricity_train.csv\n",
    "This path will later be used as the training input for the XGBoost estimator.\n",
    "\n",
    "- Test dataset upload\n",
    "The file electricity_test.csv is uploaded in the same way and stored for:\n",
    "\n",
    "- - preparation of batch transform input\n",
    "\n",
    "- - model evaluation\n",
    "\n",
    "- Returned S3 paths\n",
    "The function returns full S3 URIs, which SageMaker uses internally when launching jobs.\n",
    "\n",
    "- Print statements\n",
    "Display the final S3 locations of both files to confirm successful upload and provide visibility for later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48d5652f-b7d3-4472-827b-18d6774df81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train S3: s3://my-cloud-ai-bucket/electricity/electricity_train.csv\n",
      "Test S3 : s3://my-cloud-ai-bucket/electricity/electricity_test.csv\n"
     ]
    }
   ],
   "source": [
    "train_s3 = session.upload_data(\n",
    "    path=\"electricity_train.csv\",\n",
    "    bucket=bucket,\n",
    "    key_prefix=f\"{prefix}\"\n",
    ")\n",
    "\n",
    "test_s3_original = session.upload_data(\n",
    "    path=\"electricity_test.csv\",\n",
    "    bucket=bucket,\n",
    "    key_prefix=f\"{prefix}\"\n",
    ")\n",
    "\n",
    "print(\"Train S3:\", train_s3)\n",
    "print(\"Test S3 :\", test_s3_original)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42977220-5cdd-4413-be0d-f3af2a7d839f",
   "metadata": {},
   "source": [
    "## Prepare Batch Transform Test File — Technical Explanation\n",
    "\n",
    "This block prepares the test dataset so it can be used with Amazon SageMaker’s Batch Transform service. Batch Transform has strict input requirements (no headers, no target column, correct feature order), so we must preprocess the test file before uploading it.\n",
    "\n",
    "- Load the original test dataset\n",
    "test_df = pd.read_csv(\"electricity_test.csv\")\n",
    "Reads the full test dataset, including the target column and all engineered features.\n",
    "\n",
    "- Define the correct feature order\n",
    "The model expects inputs in the same order used during training.\n",
    "The feature_cols list ensures Batch Transform receives features in the correct sequence.\n",
    "\n",
    "- Select only numeric model features\n",
    "test_clean = test_df[feature_cols]\n",
    "Removes the target column (demand_mw) because SageMaker Batch Transform requires only input features.\n",
    "\n",
    "- Save the cleaned file without header or index\n",
    "to_csv(..., header=False, index=False)\n",
    "Batch Transform requires a pure CSV of numeric values:\n",
    "\n",
    "- no column names\n",
    "\n",
    "- no index\n",
    "\n",
    "- no extra metadata\n",
    "\n",
    "- Preview the cleaned test dataset\n",
    "print(test_clean.head())\n",
    "Displays the first rows to confirm correct formatting before uploading to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23fe936b-d9d2-4e74-97b7-95d3728c71f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared batch transform CSV:\n",
      "   hour  day_of_week  month  week  is_weekend    lag_1   lag_24  lag_168  \\\n",
      "0     0            0      1     1           0  19894.0  20527.0  17583.0   \n",
      "1     1            0      1     1           0  19912.5  19851.5  17460.0   \n",
      "2     2            0      1     1           0  19747.0  18983.0  16496.0   \n",
      "3     3            0      1     1           0  18429.0  17948.5  15535.0   \n",
      "4     4            0      1     1           0  17264.5  17436.5  15011.0   \n",
      "\n",
      "        roll_24      roll_168  \n",
      "0  23172.854167  22958.660714  \n",
      "1  23147.250000  22972.526786  \n",
      "2  23142.895833  22986.139881  \n",
      "3  23119.812500  22997.645833  \n",
      "4  23091.312500  23007.940476  \n"
     ]
    }
   ],
   "source": [
    "# Load test\n",
    "test_df = pd.read_csv(\"electricity_test.csv\")\n",
    "\n",
    "# Correct training feature order\n",
    "feature_cols = [\n",
    "    \"hour\", \"day_of_week\", \"month\", \"week\", \"is_weekend\",\n",
    "    \"lag_1\", \"lag_24\", \"lag_168\", \"roll_24\", \"roll_168\"\n",
    "]\n",
    "\n",
    "# Keep numeric features only\n",
    "test_clean = test_df[feature_cols]\n",
    "\n",
    "# Save WITHOUT header / index (SageMaker requirement)\n",
    "clean_test_path = \"electricity_test_noml.csv\"\n",
    "test_clean.to_csv(clean_test_path, header=False, index=False)\n",
    "\n",
    "print(\"Prepared batch transform CSV:\")\n",
    "print(test_clean.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cd30e2-051a-424d-92db-82320545a3c2",
   "metadata": {},
   "source": [
    "## Upload Clean Test File to S3 — Technical Explanation\n",
    "\n",
    "This block uploads the batch-ready test file to Amazon S3 so it can be used as input for the SageMaker Batch Transform job.\n",
    "\n",
    "- session.upload_data()\n",
    "Uploads the cleaned test file (electricity_test_noml.csv) to your S3 bucket under the project prefix.\n",
    "SageMaker requires all Batch Transform inputs to be stored in S3.\n",
    "\n",
    "- Why upload again?\n",
    "The original test file contained:\n",
    "\n",
    "- - the target column\n",
    "\n",
    "- - headers\n",
    "\n",
    "- - an index\n",
    "These are not allowed for Batch Transform.\n",
    "So we upload the cleaned version instead.\n",
    "\n",
    "- Returned S3 URI\n",
    "The function returns the full S3 path (e.g.,\n",
    "s3://my-cloud-ai-bucket/electricity/electricity_test_noml.csv),\n",
    "which will be passed into the Transformer for inference.\n",
    "\n",
    "- Print statement\n",
    "Shows the S3 location to confirm the upload succeeded and provide a reference for later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5fa54a2-5f16-4d3a-8943-cbf5b7cdc5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean test S3: s3://my-cloud-ai-bucket/electricity/electricity_test_noml.csv\n"
     ]
    }
   ],
   "source": [
    "test_s3 = session.upload_data(\n",
    "    path=clean_test_path,\n",
    "    bucket=bucket,\n",
    "    key_prefix=f\"{prefix}\"\n",
    ")\n",
    "\n",
    "print(\"Clean test S3:\", test_s3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6566041f-4fd4-4e04-9365-6cb50626eb14",
   "metadata": {},
   "source": [
    "## Retrieve XGBoost Training Container Image — Technical Explanation\n",
    "\n",
    "This block retrieves the correct Amazon SageMaker Docker container image for training an XGBoost model.\n",
    "\n",
    "- get_image_uri(region, \"xgboost\", \"1.5-1\")\n",
    "Requests the specific pre-built container image for:\n",
    "\n",
    "- - your current AWS region\n",
    "\n",
    "- - the XGBoost algorithm\n",
    "\n",
    "- - the 1.5-1 version of the XGBoost training container\n",
    "\n",
    "- Why this is required\n",
    "SageMaker runs training jobs inside managed containers.\n",
    "Each container includes:\n",
    "\n",
    "- - the XGBoost training binary\n",
    "\n",
    "- - dependencies\n",
    "\n",
    "- - input/output handlers\n",
    "\n",
    "- - logging tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8670e2b-5c4e-4ae3-8cbf-a95f039d0cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.deprecations:The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.5-1'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "container = get_image_uri(region, \"xgboost\", \"1.5-1\")\n",
    "container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b6fa32",
   "metadata": {},
   "source": [
    "## Configure XGBoost Estimator for SageMaker — Technical Explanation\n",
    "\n",
    "This block creates and configures the SageMaker Estimator object, which defines how the training job will run in the AWS environment.\n",
    "\n",
    "#### Estimator configuration\n",
    "\n",
    "- image_uri=container\n",
    "Uses the XGBoost training container image retrieved earlier. This tells SageMaker which algorithm and runtime environment to use.\n",
    "\n",
    "- role=role\n",
    "IAM execution role that grants SageMaker permission to:\n",
    "\n",
    "- - read input files from S3\n",
    "\n",
    "- - write model artifacts back to S3\n",
    "\n",
    "- - create training instances\n",
    "\n",
    "- instance_count=1\n",
    "Runs training on a single machine (single-node training).\n",
    "\n",
    "- instance_type=\"ml.m5.xlarge\"\n",
    "Specifies the compute instance used for training.\n",
    "ml.m5.xlarge provides 4 vCPUs and 16 GB RAM — suitable for medium-sized datasets.\n",
    "\n",
    "- volume_size=10\n",
    "Size of the attached storage (in GB) available during training for temporary files.\n",
    "\n",
    "- output_path=f\"s3://{bucket}/{prefix}/output\"\n",
    "Sets the S3 destination where the trained model artifact (model.tar.gz) will be saved.\n",
    "\n",
    "- sagemaker_session=session\n",
    "Links the estimator to your active SageMaker session for job orchestration.\n",
    "\n",
    "#### Hyperparameter configuration\n",
    "\n",
    "- objective=\"reg:squarederror\"\n",
    "Standard regression objective used for numeric prediction tasks.\n",
    "\n",
    "- num_round=150\n",
    "Number of boosting iterations (trees). More rounds increase model complexity.\n",
    "\n",
    "- max_depth=8\n",
    "Maximum depth of each decision tree — controls model flexibility.\n",
    "\n",
    "- eta=0.1\n",
    "Learning rate, determining how fast the model adapts.\n",
    "\n",
    "- subsample=0.9 and colsample_bytree=0.9\n",
    "Random row/column sampling for each tree, improving generalization and reducing overfitting.\n",
    "\n",
    "- gamma=0\n",
    "Minimum loss reduction required to make a split; controls pruning.\n",
    "\n",
    "These hyperparameters fully define how the XGBoost model will be trained in the SageMaker environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c20426b1-59b8-4c10-b38d-2efd538323dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "xgb_estimator = Estimator(\n",
    "    image_uri=container,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size=10,\n",
    "    output_path=f\"s3://{bucket}/{prefix}/output\",\n",
    "    sagemaker_session=session,\n",
    ")\n",
    "\n",
    "xgb_estimator.set_hyperparameters(\n",
    "    objective=\"reg:squarederror\",\n",
    "    num_round=150,\n",
    "    max_depth=8,\n",
    "    eta=0.1,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    gamma=0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd4f7ab",
   "metadata": {},
   "source": [
    "## Launch SageMaker Training Job — Technical Explanation\n",
    "\n",
    "This block starts the remote XGBoost training job on Amazon SageMaker using the training dataset stored in S3.\n",
    "\n",
    "- TrainingInput(train_s3, content_type=\"text/csv\")\n",
    "Wraps the training dataset S3 path in a SageMaker TrainingInput object.\n",
    "This tells SageMaker:\n",
    "\n",
    "- - where the training data is stored\n",
    "\n",
    "- - that the file format is CSV\n",
    "\n",
    "- - that the CSV has no header row\n",
    "\n",
    "- xgb_estimator.fit({\"train\": train_input})\n",
    "Launches the actual SageMaker training job.\n",
    "During this step, SageMaker will:\n",
    "\n",
    "- - create a training instance\n",
    "\n",
    "- - download the training CSV from S3\n",
    "\n",
    "- - run XGBoost training inside the container\n",
    "\n",
    "- -save model.tar.gz to the output S3 folder\n",
    "\n",
    "- print(\"Training completed.\")\n",
    "Confirms that the job finished.\n",
    "(The training job runs remotely, so this prints only after SageMaker returns control back to the notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c6f7dd0-e53f-403a-aab3-7424a2e3b40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2025-11-23-19-07-20-371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-23 19:07:21 Starting - Starting the training job...\n",
      "2025-11-23 19:07:36 Starting - Preparing the instances for training...\n",
      "2025-11-23 19:08:24 Downloading - Downloading the training image......\n",
      "2025-11-23 19:09:09 Training - Training image download completed. Training in progress.\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34m[2025-11-23 19:09:16.789 ip-10-0-70-181.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2025-11-23 19:09:16.810 ip-10-0-70-181.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:09:17:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:09:17:INFO] Failed to parse hyperparameter objective value reg:squarederror to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:09:17:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:09:17:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:09:17:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:09:17:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:09:17:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:09:17:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:09:17:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:09:17:INFO] Train matrix has 201433 rows and 11 columns\u001b[0m\n",
      "\u001b[34m[2025-11-23 19:09:17.286 ip-10-0-70-181.ec2.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2025-11-23 19:09:17.286 ip-10-0-70-181.ec2.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2025-11-23 19:09:17.287 ip-10-0-70-181.ec2.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2025-11-23 19:09:17.287 ip-10-0-70-181.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:09:17:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[34m[2025-11-23 19:09:17.454 ip-10-0-70-181.ec2.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2025-11-23 19:09:17.460 ip-10-0-70-181.ec2.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[0]#011train-rmse:1810.35852\u001b[0m\n",
      "\u001b[34m[1]#011train-rmse:1629.19507\u001b[0m\n",
      "\u001b[34m[2]#011train-rmse:1466.52234\u001b[0m\n",
      "\u001b[34m[3]#011train-rmse:1319.75781\u001b[0m\n",
      "\u001b[34m[4]#011train-rmse:1187.66345\u001b[0m\n",
      "\u001b[34m[5]#011train-rmse:1069.05701\u001b[0m\n",
      "\u001b[34m[6]#011train-rmse:962.12940\u001b[0m\n",
      "\u001b[34m[7]#011train-rmse:865.96411\u001b[0m\n",
      "\u001b[34m[8]#011train-rmse:779.28687\u001b[0m\n",
      "\u001b[34m[9]#011train-rmse:701.38599\u001b[0m\n",
      "\u001b[34m[10]#011train-rmse:631.27771\u001b[0m\n",
      "\u001b[34m[11]#011train-rmse:568.17090\u001b[0m\n",
      "\u001b[34m[12]#011train-rmse:511.42557\u001b[0m\n",
      "\u001b[34m[13]#011train-rmse:460.19586\u001b[0m\n",
      "\u001b[34m[14]#011train-rmse:414.18732\u001b[0m\n",
      "\u001b[34m[15]#011train-rmse:372.78207\u001b[0m\n",
      "\u001b[34m[16]#011train-rmse:335.51090\u001b[0m\n",
      "\u001b[34m[17]#011train-rmse:301.94904\u001b[0m\n",
      "\u001b[34m[18]#011train-rmse:271.76034\u001b[0m\n",
      "\u001b[34m[19]#011train-rmse:244.58249\u001b[0m\n",
      "\u001b[34m[20]#011train-rmse:220.14224\u001b[0m\n",
      "\u001b[34m[21]#011train-rmse:198.12178\u001b[0m\n",
      "\u001b[34m[22]#011train-rmse:178.31215\u001b[0m\n",
      "\u001b[34m[23]#011train-rmse:160.49873\u001b[0m\n",
      "\u001b[34m[24]#011train-rmse:144.44778\u001b[0m\n",
      "\u001b[34m[25]#011train-rmse:130.00925\u001b[0m\n",
      "\u001b[34m[26]#011train-rmse:117.01674\u001b[0m\n",
      "\u001b[34m[27]#011train-rmse:105.32224\u001b[0m\n",
      "\u001b[34m[28]#011train-rmse:94.79501\u001b[0m\n",
      "\u001b[34m[29]#011train-rmse:85.32609\u001b[0m\n",
      "\u001b[34m[30]#011train-rmse:76.79873\u001b[0m\n",
      "\u001b[34m[31]#011train-rmse:69.12767\u001b[0m\n",
      "\u001b[34m[32]#011train-rmse:62.22773\u001b[0m\n",
      "\u001b[34m[33]#011train-rmse:56.01418\u001b[0m\n",
      "\u001b[34m[34]#011train-rmse:50.42273\u001b[0m\n",
      "\u001b[34m[35]#011train-rmse:45.39157\u001b[0m\n",
      "\u001b[34m[36]#011train-rmse:40.86458\u001b[0m\n",
      "\u001b[34m[37]#011train-rmse:36.79137\u001b[0m\n",
      "\u001b[34m[38]#011train-rmse:33.12694\u001b[0m\n",
      "\u001b[34m[39]#011train-rmse:29.83056\u001b[0m\n",
      "\u001b[34m[40]#011train-rmse:26.86587\u001b[0m\n",
      "\u001b[34m[41]#011train-rmse:24.19742\u001b[0m\n",
      "\u001b[34m[42]#011train-rmse:21.79942\u001b[0m\n",
      "\u001b[34m[43]#011train-rmse:19.64301\u001b[0m\n",
      "\u001b[34m[44]#011train-rmse:17.70207\u001b[0m\n",
      "\u001b[34m[45]#011train-rmse:15.95828\u001b[0m\n",
      "\u001b[34m[46]#011train-rmse:14.39087\u001b[0m\n",
      "\u001b[34m[47]#011train-rmse:12.98335\u001b[0m\n",
      "\u001b[34m[48]#011train-rmse:11.71884\u001b[0m\n",
      "\u001b[34m[49]#011train-rmse:10.58352\u001b[0m\n",
      "\u001b[34m[50]#011train-rmse:9.56666\u001b[0m\n",
      "\u001b[34m[51]#011train-rmse:8.65455\u001b[0m\n",
      "\u001b[34m[52]#011train-rmse:7.83735\u001b[0m\n",
      "\u001b[34m[53]#011train-rmse:7.10715\u001b[0m\n",
      "\u001b[34m[54]#011train-rmse:6.45429\u001b[0m\n",
      "\u001b[34m[55]#011train-rmse:5.87207\u001b[0m\n",
      "\u001b[34m[56]#011train-rmse:5.35299\u001b[0m\n",
      "\u001b[34m[57]#011train-rmse:4.89431\u001b[0m\n",
      "\u001b[34m[58]#011train-rmse:4.48726\u001b[0m\n",
      "\u001b[34m[59]#011train-rmse:4.12770\u001b[0m\n",
      "\u001b[34m[60]#011train-rmse:3.80959\u001b[0m\n",
      "\u001b[34m[61]#011train-rmse:3.53160\u001b[0m\n",
      "\u001b[34m[62]#011train-rmse:3.28960\u001b[0m\n",
      "\u001b[34m[63]#011train-rmse:3.07976\u001b[0m\n",
      "\u001b[34m[64]#011train-rmse:2.89454\u001b[0m\n",
      "\u001b[34m[65]#011train-rmse:2.73772\u001b[0m\n",
      "\u001b[34m[66]#011train-rmse:2.59260\u001b[0m\n",
      "\u001b[34m[67]#011train-rmse:2.47629\u001b[0m\n",
      "\u001b[34m[68]#011train-rmse:2.37592\u001b[0m\n",
      "\u001b[34m[69]#011train-rmse:2.29172\u001b[0m\n",
      "\u001b[34m[70]#011train-rmse:2.22130\u001b[0m\n",
      "\u001b[34m[71]#011train-rmse:2.15916\u001b[0m\n",
      "\u001b[34m[72]#011train-rmse:2.10890\u001b[0m\n",
      "\u001b[34m[73]#011train-rmse:2.06540\u001b[0m\n",
      "\u001b[34m[74]#011train-rmse:2.02420\u001b[0m\n",
      "\u001b[34m[75]#011train-rmse:1.98728\u001b[0m\n",
      "\u001b[34m[76]#011train-rmse:1.95765\u001b[0m\n",
      "\u001b[34m[77]#011train-rmse:1.93300\u001b[0m\n",
      "\u001b[34m[78]#011train-rmse:1.91463\u001b[0m\n",
      "\u001b[34m[79]#011train-rmse:1.89471\u001b[0m\n",
      "\u001b[34m[80]#011train-rmse:1.88164\u001b[0m\n",
      "\u001b[34m[81]#011train-rmse:1.86628\u001b[0m\n",
      "\u001b[34m[82]#011train-rmse:1.85607\u001b[0m\n",
      "\u001b[34m[83]#011train-rmse:1.84493\u001b[0m\n",
      "\u001b[34m[84]#011train-rmse:1.83560\u001b[0m\n",
      "\u001b[34m[85]#011train-rmse:1.82858\u001b[0m\n",
      "\u001b[34m[86]#011train-rmse:1.82264\u001b[0m\n",
      "\u001b[34m[87]#011train-rmse:1.81313\u001b[0m\n",
      "\u001b[34m[88]#011train-rmse:1.80839\u001b[0m\n",
      "\u001b[34m[89]#011train-rmse:1.80180\u001b[0m\n",
      "\u001b[34m[90]#011train-rmse:1.79466\u001b[0m\n",
      "\u001b[34m[91]#011train-rmse:1.78171\u001b[0m\n",
      "\u001b[34m[92]#011train-rmse:1.76910\u001b[0m\n",
      "\u001b[34m[93]#011train-rmse:1.76306\u001b[0m\n",
      "\u001b[34m[94]#011train-rmse:1.75959\u001b[0m\n",
      "\u001b[34m[95]#011train-rmse:1.75426\u001b[0m\n",
      "\u001b[34m[96]#011train-rmse:1.74293\u001b[0m\n",
      "\u001b[34m[97]#011train-rmse:1.73687\u001b[0m\n",
      "\u001b[34m[98]#011train-rmse:1.73362\u001b[0m\n",
      "\u001b[34m[99]#011train-rmse:1.72607\u001b[0m\n",
      "\u001b[34m[100]#011train-rmse:1.72406\u001b[0m\n",
      "\u001b[34m[101]#011train-rmse:1.71952\u001b[0m\n",
      "\u001b[34m[102]#011train-rmse:1.71561\u001b[0m\n",
      "\u001b[34m[103]#011train-rmse:1.71106\u001b[0m\n",
      "\u001b[34m[104]#011train-rmse:1.70741\u001b[0m\n",
      "\u001b[34m[105]#011train-rmse:1.70429\u001b[0m\n",
      "\u001b[34m[106]#011train-rmse:1.69365\u001b[0m\n",
      "\u001b[34m[107]#011train-rmse:1.69200\u001b[0m\n",
      "\u001b[34m[108]#011train-rmse:1.68718\u001b[0m\n",
      "\u001b[34m[109]#011train-rmse:1.68526\u001b[0m\n",
      "\u001b[34m[110]#011train-rmse:1.68085\u001b[0m\n",
      "\u001b[34m[111]#011train-rmse:1.67962\u001b[0m\n",
      "\u001b[34m[112]#011train-rmse:1.67450\u001b[0m\n",
      "\u001b[34m[113]#011train-rmse:1.66961\u001b[0m\n",
      "\u001b[34m[114]#011train-rmse:1.66667\u001b[0m\n",
      "\u001b[34m[115]#011train-rmse:1.66450\u001b[0m\n",
      "\u001b[34m[116]#011train-rmse:1.65855\u001b[0m\n",
      "\u001b[34m[117]#011train-rmse:1.65514\u001b[0m\n",
      "\u001b[34m[118]#011train-rmse:1.64493\u001b[0m\n",
      "\u001b[34m[119]#011train-rmse:1.63964\u001b[0m\n",
      "\u001b[34m[120]#011train-rmse:1.63801\u001b[0m\n",
      "\u001b[34m[121]#011train-rmse:1.63537\u001b[0m\n",
      "\u001b[34m[122]#011train-rmse:1.62985\u001b[0m\n",
      "\u001b[34m[123]#011train-rmse:1.62696\u001b[0m\n",
      "\u001b[34m[124]#011train-rmse:1.62095\u001b[0m\n",
      "\u001b[34m[125]#011train-rmse:1.61236\u001b[0m\n",
      "\u001b[34m[126]#011train-rmse:1.60972\u001b[0m\n",
      "\u001b[34m[127]#011train-rmse:1.59810\u001b[0m\n",
      "\u001b[34m[128]#011train-rmse:1.59532\u001b[0m\n",
      "\u001b[34m[129]#011train-rmse:1.59212\u001b[0m\n",
      "\u001b[34m[130]#011train-rmse:1.58725\u001b[0m\n",
      "\u001b[34m[131]#011train-rmse:1.58467\u001b[0m\n",
      "\u001b[34m[132]#011train-rmse:1.58073\u001b[0m\n",
      "\u001b[34m[133]#011train-rmse:1.57659\u001b[0m\n",
      "\u001b[34m[134]#011train-rmse:1.57315\u001b[0m\n",
      "\u001b[34m[135]#011train-rmse:1.56922\u001b[0m\n",
      "\u001b[34m[136]#011train-rmse:1.56523\u001b[0m\n",
      "\u001b[34m[137]#011train-rmse:1.56160\u001b[0m\n",
      "\u001b[34m[138]#011train-rmse:1.56122\u001b[0m\n",
      "\u001b[34m[139]#011train-rmse:1.55831\u001b[0m\n",
      "\u001b[34m[140]#011train-rmse:1.55696\u001b[0m\n",
      "\u001b[34m[141]#011train-rmse:1.55407\u001b[0m\n",
      "\u001b[34m[142]#011train-rmse:1.55230\u001b[0m\n",
      "\u001b[34m[143]#011train-rmse:1.54738\u001b[0m\n",
      "\u001b[34m[144]#011train-rmse:1.54238\u001b[0m\n",
      "\u001b[34m[145]#011train-rmse:1.53326\u001b[0m\n",
      "\u001b[34m[146]#011train-rmse:1.52916\u001b[0m\n",
      "\u001b[34m[147]#011train-rmse:1.52651\u001b[0m\n",
      "\u001b[34m[148]#011train-rmse:1.52060\u001b[0m\n",
      "\u001b[34m[149]#011train-rmse:1.51796\u001b[0m\n",
      "\n",
      "2025-11-23 19:10:13 Uploading - Uploading generated training model\n",
      "2025-11-23 19:10:13 Completed - Training job completed\n",
      "Training seconds: 135\n",
      "Billable seconds: 135\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "train_input = TrainingInput(train_s3, content_type=\"text/csv\")\n",
    "\n",
    "xgb_estimator.fit({\"train\": train_input})\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01727b5e-674a-4ba5-8b1e-fd25c062f3ed",
   "metadata": {},
   "source": [
    "## Retrieve Model Artifact Details — Technical Explanation\n",
    "\n",
    "This block accesses the most recent SageMaker training job created by the XGBoost estimator and extracts the S3 location where the trained model artifact was saved.\n",
    "\n",
    "- training_job = xgb_estimator.latest_training_job\n",
    "Retrieves a handle to the last training job launched by the estimator.\n",
    "This object provides metadata and status information for that specific job.\n",
    "\n",
    "- desc = training_job.describe()\n",
    "Calls the SageMaker API to obtain a full job description.\n",
    "The returned dictionary includes:\n",
    "\n",
    "- - training job configuration\n",
    "\n",
    "- - input/output locations\n",
    "\n",
    "- - hyperparameters\n",
    "\n",
    "- - resource usage\n",
    "\n",
    "- - model artifact paths\n",
    "\n",
    "- model_artifact = desc[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "Navigates the job description to extract the S3 URI containing the trained model artifact.\n",
    "SageMaker packages the trained model as model.tar.gz and stores it in this location.\n",
    "\n",
    "- print(\"Model artifacts:\", model_artifact)\n",
    "Displays the extracted S3 path so it can be used in later steps such as model creation, deployment, or batch transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88011fd0-75f8-46b3-adb0-f2152b9f9ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model artifacts: s3://my-cloud-ai-bucket/electricity/output/sagemaker-xgboost-2025-11-23-19-07-20-371/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "training_job = xgb_estimator.latest_training_job\n",
    "desc = training_job.describe()\n",
    "\n",
    "model_artifact = desc[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "print(\"Model artifacts:\", model_artifact)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f602f41c-acc3-4c6b-98f7-3c8bad13548d",
   "metadata": {},
   "source": [
    "## Create a SageMaker Model Object — Technical Explanation\n",
    "\n",
    "This block registers the trained model artifact with SageMaker by creating a Model object that links the trained model file, the inference container, and the execution role.\n",
    "\n",
    "- from sagemaker.model import Model\n",
    "Imports the Model class, which represents a deployable model entity inside SageMaker.\n",
    "\n",
    "- model_name = \"electricity-xgb-model\"\n",
    "Defines a unique name that identifies the model within the SageMaker environment.\n",
    "\n",
    "- model = Model(...)\n",
    "Constructs a SageMaker Model object with the following components:\n",
    "\n",
    "- - name=model_name: Assigns the chosen model name.\n",
    "\n",
    "- - model_data=model_artifact: Points to the S3 URI of the trained model.tar.gz.\n",
    "\n",
    "- - image_uri=container: Specifies the XGBoost inference container image.\n",
    "\n",
    "- - role=role: Provides SageMaker permission to load the model and run inference.\n",
    "\n",
    "- model.create(instance_type=\"ml.m5.large\")\n",
    "Registers the model in SageMaker so it can be used for deployment or batch transform.\n",
    "The instance type is specified for validation, but this does not start a real endpoint.\n",
    "\n",
    "- print(\"Model created:\", model_name)\n",
    "Confirms successful model registration within the SageMaker environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33903828-9683-4bb8-ba21-c13fbf6ee2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: electricity-xgb-model\n",
      "WARNING:sagemaker:Using already existing model: electricity-xgb-model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created: electricity-xgb-model\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "model_name = \"electricity-xgb-model\"\n",
    "\n",
    "model = Model(\n",
    "    name=model_name,\n",
    "    model_data=model_artifact,\n",
    "    image_uri=container,\n",
    "    role=role\n",
    ")\n",
    "\n",
    "model.create(instance_type=\"ml.m5.large\")\n",
    "print(\"Model created:\", model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a3be4f-c80a-4854-af55-44e1c4b81198",
   "metadata": {},
   "source": [
    "## Run SageMaker Batch Transform Job — Technical Explanation\n",
    "\n",
    "This block creates and executes a Batch Transform job using the previously registered SageMaker model. Batch Transform performs large-scale offline inference by loading the model onto temporary compute instances and processing the test dataset stored in S3.\n",
    "\n",
    "- from sagemaker.transformer import Transformer\n",
    "Imports the Transformer class, which is used to configure and run batch inference jobs on SageMaker.\n",
    "\n",
    "- transformer = Transformer(...)\n",
    "Creates a Transformer object with the following parameters:\n",
    "\n",
    "  - model_name=model_name: Specifies which registered SageMaker model to load for inference.\n",
    "\n",
    "  - instance_count=1: Runs the batch job on a single compute instance.\n",
    "\n",
    "  - instance_type=\"ml.m5.large\": Defines the hardware used for inference.\n",
    " \n",
    "  - output_path=f\"s3://{bucket}/{prefix}/batch_output\": Determines the S3 folder where prediction results will be stored.\n",
    "\n",
    "- transformer.transform(...)\n",
    "Launches the batch transform job and provides the job-specific configuration:\n",
    "\n",
    "  - data=test_s3: S3 path to the cleaned test dataset.\n",
    "\n",
    "  - content_type=\"text/csv\": Indicates the input file format.\n",
    "\n",
    "  - split_type=\"Line\": Processes the dataset one line (row) at a time.\n",
    "\n",
    "- transformer.wait()\n",
    "Blocks execution until the batch transform job completes.\n",
    "During this process, SageMaker:\n",
    "\n",
    "  - pulls the model\n",
    "\n",
    "  - spins up an instance\n",
    "\n",
    "  - runs inference on the dataset\n",
    "\n",
    "  - stores predictions in the output S3 folder\n",
    "\n",
    "- print(\"Batch transform done.\")\n",
    "Confirms that SageMaker successfully completed inference on the entire test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5dba339b-d062-483c-bad0-abb64bad0e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: sagemaker-xgboost-2025-11-23-19-10-39-321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running batch transform...\n",
      ".................................\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:16:14:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:16:14:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:16:14:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2025-11-23 19:16:14 +0000] [14] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[34m[2025-11-23 19:16:14 +0000] [14] [INFO] Listening at: unix:/tmp/gunicorn.sock (14)\u001b[0m\n",
      "\u001b[34m[2025-11-23 19:16:14 +0000] [14] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/os.py:1023: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  return io.open(fd, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[2025-11-23 19:16:14 +0000] [18] [INFO] Booting worker with pid: 18\u001b[0m\n",
      "\u001b[34m[2025-11-23 19:16:14 +0000] [19] [INFO] Booting worker with pid: 19\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:16:16:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:16:16:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:16:17:INFO] Model objective : reg:squarederror\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:16:17:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:16:17:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:16:17:INFO] Model objective : reg:squarederror\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:16:23:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [23/Nov/2025:19:16:23 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:16:23:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [23/Nov/2025:19:16:23 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:16:23:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/core.py:105: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m[2025-11-23:19:16:23:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [23/Nov/2025:19:16:23 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2025-11-23:19:16:23:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [23/Nov/2025:19:16:23 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2025-11-23:19:16:23:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.8/site-packages/xgboost/core.py:105: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [23/Nov/2025:19:16:24 +0000] \"POST /invocations HTTP/1.1\" 200 291006 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [23/Nov/2025:19:16:24 +0000] \"POST /invocations HTTP/1.1\" 200 291006 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2025-11-23T19:16:23.367:[sagemaker logs]: MaxConcurrentTransforms=2, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:16:14:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:16:14:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:16:14:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[35m[2025-11-23:19:16:14:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2025-11-23:19:16:14:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2025-11-23:19:16:14:INFO] nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2025-11-23 19:16:14 +0000] [14] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[34m[2025-11-23 19:16:14 +0000] [14] [INFO] Listening at: unix:/tmp/gunicorn.sock (14)\u001b[0m\n",
      "\u001b[34m[2025-11-23 19:16:14 +0000] [14] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/os.py:1023: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  return io.open(fd, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[2025-11-23 19:16:14 +0000] [18] [INFO] Booting worker with pid: 18\u001b[0m\n",
      "\u001b[34m[2025-11-23 19:16:14 +0000] [19] [INFO] Booting worker with pid: 19\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[2025-11-23 19:16:14 +0000] [14] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[35m[2025-11-23 19:16:14 +0000] [14] [INFO] Listening at: unix:/tmp/gunicorn.sock (14)\u001b[0m\n",
      "\u001b[35m[2025-11-23 19:16:14 +0000] [14] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.8/os.py:1023: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  return io.open(fd, *args, **kwargs)\u001b[0m\n",
      "\u001b[35m[2025-11-23 19:16:14 +0000] [18] [INFO] Booting worker with pid: 18\u001b[0m\n",
      "\u001b[35m[2025-11-23 19:16:14 +0000] [19] [INFO] Booting worker with pid: 19\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:16:16:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:16:16:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:16:17:INFO] Model objective : reg:squarederror\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:16:17:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:16:17:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:16:17:INFO] Model objective : reg:squarederror\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[35m[2025-11-23:19:16:16:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2025-11-23:19:16:16:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2025-11-23:19:16:17:INFO] Model objective : reg:squarederror\u001b[0m\n",
      "\u001b[35m[2025-11-23:19:16:17:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2025-11-23:19:16:17:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2025-11-23:19:16:17:INFO] Model objective : reg:squarederror\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:16:23:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [23/Nov/2025:19:16:23 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:16:23:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [23/Nov/2025:19:16:23 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2025-11-23:19:16:23:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/core.py:105: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m[2025-11-23:19:16:23:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [23/Nov/2025:19:16:23 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2025-11-23:19:16:23:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [23/Nov/2025:19:16:23 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2025-11-23:19:16:23:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.8/site-packages/xgboost/core.py:105: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [23/Nov/2025:19:16:24 +0000] \"POST /invocations HTTP/1.1\" 200 291006 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [23/Nov/2025:19:16:24 +0000] \"POST /invocations HTTP/1.1\" 200 291006 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2025-11-23T19:16:23.367:[sagemaker logs]: MaxConcurrentTransforms=2, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "Batch transform done.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "transformer = Transformer(\n",
    "    model_name=model_name,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=f\"s3://{bucket}/{prefix}/batch_output\"\n",
    ")\n",
    "\n",
    "print(\"Running batch transform...\")\n",
    "\n",
    "transformer.transform(\n",
    "    data=test_s3,\n",
    "    content_type=\"text/csv\",\n",
    "    split_type=\"Line\"\n",
    ")\n",
    "\n",
    "transformer.wait()\n",
    "print(\"Batch transform done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c51d35-97bc-4424-8315-3e6919139379",
   "metadata": {},
   "source": [
    "## Download and Load Batch Transform Predictions — Technical Explanation\n",
    "\n",
    "This block retrieves the prediction output generated by the SageMaker Batch Transform job, downloads it from S3, and loads it into a pandas DataFrame for inspection.\n",
    "\n",
    "- s3 = boto3.client(\"s3\")\n",
    "  Creates a low-level S3 client using boto3, enabling direct file operations such as downloads.\n",
    "\n",
    "- output_key = f\"{prefix}/batch_output/{clean_test_path}.out\"\n",
    "  Constructs the exact S3 key where Batch Transform stored its output file.\n",
    "  SageMaker automatically appends .out to the processed input filename.\n",
    "\n",
    "- local_out = \"predictions.csv\"\n",
    "  Specifies the local filename where the downloaded predictions will be saved.\n",
    "\n",
    "- s3.download_file(bucket, output_key, local_out)\n",
    "  Downloads the Batch Transform result from S3 to the local environment.\n",
    "  The file contains one prediction per line, matching the order of the input test dataset.\n",
    "\n",
    "- preds = pd.read_csv(local_out, header=None)\n",
    "  Loads the downloaded predictions into a DataFrame.\n",
    "  header=None is required because Batch Transform outputs raw values without header rows.\n",
    "\n",
    "- preds.columns = [\"prediction\"]\n",
    "  Assigns a meaningful column name to the predictions for easier interpretation and later evaluation.\n",
    "\n",
    "- print(preds.head())\n",
    "  Displays the first few predictions to verify that the Batch Transform output was loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "41ce1506-9397-44e8-8066-838a206b57bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    prediction\n",
      "0  1692.887939\n",
      "1  1692.887939\n",
      "2  1692.718262\n",
      "3  1692.718262\n",
      "4  1692.718262\n"
     ]
    }
   ],
   "source": [
    "# Download output\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "output_key = f\"{prefix}/batch_output/{clean_test_path}.out\"\n",
    "local_out = \"predictions.csv\"\n",
    "\n",
    "s3.download_file(bucket, output_key, local_out)\n",
    "\n",
    "preds = pd.read_csv(local_out, header=None)\n",
    "preds.columns = [\"prediction\"]\n",
    "\n",
    "print(preds.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ec3d3c",
   "metadata": {},
   "source": [
    "## Evaluate Predictions Using RMSE — Technical Explanation\n",
    "\n",
    "This block loads the original test dataset containing the true target values and compares them to the predictions produced by the SageMaker Batch Transform job. The evaluation metric used here is RMSE (Root Mean Squared Error), which measures how far the predictions deviate from the actual electricity demand values.\n",
    "\n",
    "- test_full = pd.read_csv(\"electricity_test.csv\")\n",
    "Loads the original test dataset from disk.\n",
    "This dataset still contains the true demand values (demand_mw) that were removed before running Batch Transform.\n",
    "\n",
    "- y_true = test_full[\"demand_mw\"]\n",
    "Extracts the actual target values so they can be compared against the predicted values.\n",
    "\n",
    "- Compute RMSE\n",
    "rmse = np.sqrt(np.mean((preds[\"prediction\"] - y_true) ** 2))\n",
    "RMSE penalizes large errors heavily, making it a useful metric for regression tasks.\n",
    "\n",
    "- rmse\n",
    "Displays the computed RMSE value, providing a quantitative measure of model performance on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59a37102-d594-4dc1-8470-2c8bb14fe092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22730.11164378932"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full test includes actual values\n",
    "test_full = pd.read_csv(\"electricity_test.csv\")\n",
    "y_true = test_full[\"demand_mw\"]\n",
    "\n",
    "rmse = np.sqrt(np.mean((preds[\"prediction\"] - y_true) ** 2))\n",
    "rmse\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
